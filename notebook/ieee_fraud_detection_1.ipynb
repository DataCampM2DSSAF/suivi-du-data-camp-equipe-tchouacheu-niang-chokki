{"cells":[{"metadata":{},"cell_type":"markdown","source":"<h1 align=\"center\" style=\"color:#6699ff\"> DataCamp IEEE Fraud Detection </h1>"},{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://github.com/DataCampM2DSSAF/suivi-du-data-camp-equipe-tchouacheu_toure_niang_chokki/blob/master/img/credit-card-fraud-detection.png?raw=true\" width=\"800\" align=\"right\">"},{"metadata":{},"cell_type":"markdown","source":"#  <a style=\"color:#6699ff\"> Team </a>\n- <a style=\"color:#6699ff\">Mohamed NIANG </a>\n- <a style=\"color:#6699ff\">Fernanda Tchouacheu </a>\n- <a style=\"color:#6699ff\">Sokhna Penda Toure </a>\n- <a style=\"color:#6699ff\">Hypolite Chokki </a>"},{"metadata":{},"cell_type":"markdown","source":"# <a style=\"color:#6699ff\">  Table of Contents</a> \n\n<a style=\"color:#6699ff\"> I. Introduction</a>\n\n<a style=\"color:#6699ff\"> II. Descriptive Statistics & Visualization</a>\n\n<a style=\"color:#6699ff\"> III. Preprocessing</a>\n\n<a style=\"color:#6699ff\"> IV. Machine Learning Models</a>"},{"metadata":{},"cell_type":"markdown","source":"# <a style=\"color:#6699ff\"> I. Introduction</a>"},{"metadata":{},"cell_type":"markdown","source":"**Pourquoi la détection de fraude ?**\n> La fraude est un commerce d'un milliard de dollars et elle augmente chaque année. L'enquête mondiale de PwC sur la criminalité économique de 2018 a révélé que la moitié (49 %) des 7 200 entreprises interrogées avaient été victimes d'une fraude quelconque. C'est une augmentation par rapport à l'étude PwC de 2016, dans laquelle un peu plus d'un tiers des organisations interrogées (36 %) avaient été victimes de la criminalité économique.\n\n\nCette compétition est un problème de **classification binaire** - c'est-à-dire que notre variable cible est un attribut binaire (l'utilisateur qui fait le clic est-il frauduleux ou non ?) et notre objectif est de classer les utilisateurs en \"frauduleux\" ou \"non frauduleux\" le mieux possible."},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file \n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\nfrom xgboost import XGBClassifier\nimport xgboost as xgb\nimport lightgbm as lgb\nimport catboost\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.metrics import roc_auc_score\nimport matplotlib.gridspec as gridspec\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport gc\ngc.enable()\n\nimport os\nos.chdir('/kaggle/input/ieeecis-fraud-detection') # Set working directory\nprint(os.listdir('/kaggle/input/ieeecis-fraud-detection'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <a style=\"color:#6699ff\"> II. Descriptive Statistics and Visualization</a>"},{"metadata":{},"cell_type":"markdown","source":"Dans cette compétition, nous voulons prédire la probabilité qu'une transaction en ligne soit frauduleuse, comme le montre la cible binaire \"isFraud\".\n\nLes données sont divisées en deux fichiers **identity** et **transaction**, qui sont reliés par \"TransactionID\". \n\n> Note : Toutes les transactions n'ont pas d'informations d'identité correspondantes.\n\n**Variables catégorielles - Transaction**\n\n- ProductCD\n- emaildomain\n- card1 - card6\n- addr1, addr2\n- P_emaildomain\n- R_emaildomain\n- M1 - M9\n\n**Variables catégorielles - Identité**\n\n- DeviceType\n- DeviceInfo\n- id_12 - id_38\n\n**La variable TransactionDT est le temps d'une date-heure de référence donnée (en seconde).**\n\n**Fichiers**\n\n- train_{transaction, identité}.csv - l'ensemble d'apprentissage\n- test_{transaction, identité}.csv - l'ensemble de test (**nous devons prédire la valeur isFraud pour ces observations**)\n- sample_submission.csv - un exemple de fichier de soumission dans le format correct\n\n> Questions\n\nNous allons commencer à explorer les données en se basant sur les caractéristiques catégorielles et les montants des transactions. L'objectif est de répondre à des questions comme :\n\n1. Quel type de données avons-nous ?\n2. Combien de cols, de lignes, de valeurs manquantes avons-nous ?\n3. Quelle est la distribution cible ?\n4. Quelle est la distribution des valeurs des transactions de fraude et des transactions sans fraude ?\n5. Avons nous des produits frauduleux prédominants ?\n6. Quelles sont les caractéristiques ou la cible qui présentent des schémas intéressants ?\n\nEt beaucoup d'autres questions qui vont soulever l'exploration."},{"metadata":{},"cell_type":"markdown","source":"**Load data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntrain_transaction = pd.read_csv('train_transaction.csv', index_col='TransactionID')\ntest_transaction = pd.read_csv('test_transaction.csv', index_col='TransactionID')\ntrain_identity = pd.read_csv('train_identity.csv', index_col='TransactionID')\ntest_identity = pd.read_csv('test_identity.csv', index_col='TransactionID')\nprint (\"Data is loaded!\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('train_transaction shape is {}'.format(train_transaction.shape))\nprint('test_transaction shape is {}'.format(test_transaction.shape))\nprint('train_identity shape is {}'.format(train_identity.shape))\nprint('test_identity shape is {}'.format(test_identity.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_transaction.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_identity.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Missing Data Problem**"},{"metadata":{},"cell_type":"markdown","source":"On remarque qu'il y a beaucoup de colonnes **NaN** : \n\n- ``` V300 ... V339``` \n- ``` id_01 ... id_34``` "},{"metadata":{},"cell_type":"markdown","source":"## 1er problème : NaN"},{"metadata":{},"cell_type":"markdown","source":"**train_transaction**"},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_values_count = train_transaction.isnull().sum()\nprint (missing_values_count[0:10])\ntotal_cells = np.product(train_transaction.shape)\ntotal_missing = missing_values_count.sum()\nprint (\"% of missing data = \",(total_missing/total_cells) * 100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**train_identity**"},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_values_count = train_identity.isnull().sum()\nprint (missing_values_count[0:10])\ntotal_cells = np.product(train_identity.shape)\ntotal_missing = missing_values_count.sum()\nprint (\"% of missing data = \",(total_missing/total_cells) * 100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del missing_values_count, total_cells, total_missing\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Imbalanced Problem**"},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.countplot(y=\"isFraud\", data=train_transaction)\nplt.title('Distribution of  isFraud')\n\ntotal = len(train_transaction['isFraud'])\nfor p in ax.patches:\n        percentage = '{:.1f}%'.format(100 * p.get_width()/total)\n        x = p.get_x() + p.get_width() + 0.02\n        y = p.get_y() + p.get_height()/2\n        ax.annotate(percentage, (x, y))\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2ème problème : Imbalanced class\n\nNous remarquons que notre jeu de données (\"isFraud\") est déséquilibrée. La plupart des transactions sont non frauduleuses. Si nous utilisons ce cadre de données comme base pour nos modèles prédictifs et nos analyses, nous pourrions obtenir beaucoup d'erreurs et nos algorithmes seront probablement surdimensionnés puisqu'ils \"supposeront\" que la plupart des transactions ne sont pas des fraudes. Mais nous ne voulons pas que notre modèle suppose, nous voulons que notre modèle détecte des modèles qui donnent des signes de fraude !\n\n**Déséquilibrée** signifie que le nombre de points de données disponibles pour les différentes classes est différent."},{"metadata":{},"cell_type":"markdown","source":"Dans cette partie, nous essayerons de voir, d'une part, ce que réprésente réellement certaines variables,\net d'autre part, s'il y a des liens entre les variables et la variable cible."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_transaction['TransactionDT'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_transaction['TransactionDT'].shape[0] , train_transaction['TransactionDT'].nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_transaction['TransactionDT'].value_counts().head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Distribution Plots"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1, 2, figsize=(18,4))\n\ntime_val = train_transaction['TransactionDT'].values\n\nsns.distplot(time_val, ax=ax[0], color='r')\nax[0].set_title('Distribution of TransactionDT', fontsize=14)\nax[1].set_xlim([min(time_val), max(time_val)])\n\nsns.distplot(np.log(time_val), ax=ax[1], color='b')\nax[1].set_title('Distribution of LOG TransactionDT', fontsize=14)\nax[1].set_xlim([min(np.log(time_val)), max(np.log(time_val))])\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1, 2, figsize=(18,4))\n\ntime_val = train_transaction.loc[train_transaction['isFraud'] == 1]['TransactionDT'].values\n\nsns.distplot(np.log(time_val), ax=ax[0], color='r')\nax[0].set_title('Distribution of LOG TransactionDT, isFraud=1', fontsize=14)\nax[1].set_xlim([min(np.log(time_val)), max(np.log(time_val))])\n\ntime_val = train_transaction.loc[train_transaction['isFraud'] == 0]['TransactionDT'].values\n\nsns.distplot(np.log(time_val), ax=ax[1], color='b')\nax[1].set_title('Distribution of LOG TransactionDT, isFraud=0', fontsize=14)\nax[1].set_xlim([min(np.log(time_val)), max(np.log(time_val))])\n\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_transaction['TransactionDT'].plot(kind='hist',\n                                        figsize=(15, 5),\n                                        label='train',\n                                        bins=50,\n                                        title='Train vs Test TransactionDT distribution')\ntest_transaction['TransactionDT'].plot(kind='hist',\n                                       label='test',\n                                       bins=50)\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del fig, ax, time_val\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_transaction.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**isFraud vs time**"},{"metadata":{"trusted":true},"cell_type":"code","source":"i = 'isFraud'\ncor = np.corrcoef(train_transaction['TransactionDT'], train_transaction[i])[0,1]\ntrain_transaction.loc[train_transaction['isFraud'] == 0].set_index('TransactionDT')[i].plot(style='.', title=i+\" corr= \"+str(round(cor,3)), figsize=(15, 3), label=\"isFraud=0\")\ntrain_transaction.loc[train_transaction['isFraud'] == 1].set_index('TransactionDT')[i].plot(style='.', title=i+\" corr= \"+str(round(cor,3)), figsize=(15, 3), label=\"isFraud=1\")\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**C features: C1, C2 ... C14**"},{"metadata":{"trusted":true},"cell_type":"code","source":"c_features = list(train_transaction.columns[16:30])\nfor i in c_features:\n    cor = np.corrcoef(train_transaction['TransactionDT'], train_transaction[i])[0,1]\n    train_transaction.set_index('TransactionDT')[i].plot(style='.', title=i+\" corr= \"+str(round(cor,3)), figsize=(15, 3))\n    test_transaction.set_index('TransactionDT')[i].plot(style='.', title=i+\" corr= \"+str(round(cor,3)), figsize=(15, 3))\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del cor, c_features\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**D features: D1 ... D15**"},{"metadata":{"trusted":true},"cell_type":"code","source":"d_features = list(train_transaction.columns[30:45])\n\nfor i in d_features:\n    cor = np.corrcoef(train_transaction['TransactionDT'], train_transaction[i])[0,1]\n    train_transaction.set_index('TransactionDT')[i].plot(style='.', title=i+\" corr= \"+str(round(cor,3)), figsize=(15, 3))\n    test_transaction.set_index('TransactionDT')[i].plot(style='.', title=i+\" corr= \"+str(round(cor,3)), figsize=(15, 3))\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Le problème ici est que les caractéristiques D sont principalement des NaNs."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_transaction[d_features].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Click output to see the number of missing values in each column\nmissing_values_count = train_transaction[d_features].isnull().sum()\nmissing_values_count","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# how many total missing values do we have?\ntotal_cells = np.product(train_transaction[d_features].shape)\ntotal_missing = missing_values_count.sum()\n# percent of data that is missing\n(total_missing/total_cells) * 100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Si nous considérons les caractéristiques D, 58.15% sont des valeurs manquantes."},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in d_features:\n    cor_tr = np.corrcoef(train_transaction['TransactionDT'], train_transaction[i].fillna(-1))[0,1]\n    cor_te = np.corrcoef(test_transaction['TransactionDT'], test_transaction[i].fillna(-1))[0,1]\n    train_transaction.set_index('TransactionDT')[i].fillna(-1).plot(style='.', title=i+\" corr_tr= \"+str(round(cor_tr,3))+\" || corr_te= \"+str(round(cor_te,3)), figsize=(15, 3))\n    test_transaction.set_index('TransactionDT')[i].fillna(-1).plot(style='.', title=i+\" corr_tr= \"+str(round(cor_tr,3))+\"  || corr_te= \"+str(round(cor_te,3)), figsize=(15, 3))\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del d_features, cor, missing_values_count, total_cells, total_missing\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**M features: M1 .. M9**"},{"metadata":{"trusted":true},"cell_type":"code","source":"m_features = list(train_transaction.columns[45:54])\ntrain_transaction[m_features].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**V150**"},{"metadata":{"trusted":true},"cell_type":"code","source":"i = \"V150\"\ncor_tr = np.corrcoef(train_transaction['TransactionDT'], train_transaction[i].fillna(-1))[0,1]\ncor_te = np.corrcoef(test_transaction['TransactionDT'], test_transaction[i].fillna(-1))[0,1]\ntrain_transaction.set_index('TransactionDT')[i].fillna(-1).plot(style='.', title=i+\" corr_tr= \"+str(round(cor_tr,3))+\" || corr_te= \"+str(round(cor_te,3)), figsize=(15, 3))\ntest_transaction.set_index('TransactionDT')[i].fillna(-1).plot(style='.', title=i+\" corr_tr= \"+str(round(cor_tr,3))+\"  || corr_te= \"+str(round(cor_te,3)), figsize=(15, 3))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del cor_tr, cor_te\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Vérifions toute la liste des V qui sont nulles."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_transaction.loc[:,train_transaction.columns[train_transaction.columns.str.startswith('V')]].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**TransactionAmt**"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1, 2, figsize=(18,4))\n\ntime_val = train_transaction['TransactionAmt'].values\n\nsns.distplot(time_val, ax=ax[0], color='r')\nax[0].set_title('Distribution of TransactionAmt', fontsize=14)\nax[1].set_xlim([min(time_val), max(time_val)])\n\nsns.distplot(np.log(time_val), ax=ax[1], color='b')\nax[1].set_title('Distribution of LOG TransactionAmt', fontsize=14)\nax[1].set_xlim([min(np.log(time_val)), max(np.log(time_val))])\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1, 2, figsize=(18,4))\n\ntime_val = train_transaction.loc[train_transaction['isFraud'] == 1]['TransactionAmt'].values\n\nsns.distplot(np.log(time_val), ax=ax[0], color='r')\nax[0].set_title('Distribution of LOG TransactionAmt, isFraud=1', fontsize=14)\nax[1].set_xlim([min(np.log(time_val)), max(np.log(time_val))])\n\ntime_val = train_transaction.loc[train_transaction['isFraud'] == 0]['TransactionAmt'].values\n\nsns.distplot(np.log(time_val), ax=ax[1], color='b')\nax[1].set_title('Distribution of LOG TransactionAmt, isFraud=0', fontsize=14)\nax[1].set_xlim([min(np.log(time_val)), max(np.log(time_val))])\n\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del fig, ax, time_val\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Count Plots"},{"metadata":{},"cell_type":"markdown","source":"**D Features**"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10, 7))\nd_features = list(train_transaction.columns[30:45])\nuniques = [len(train_transaction[col].unique()) for col in d_features]\nsns.set(font_scale=1.2)\nax = sns.barplot(d_features, uniques, log=True)\nax.set(xlabel='Feature', ylabel='log(unique count)', title='Number of unique values per feature TRAIN')\nfor p, uniq in zip(ax.patches, uniques):\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()/2.,\n            height + 10,\n            uniq,\n            ha=\"center\") ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del d_features, uniques\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**C features**"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10, 7))\nc_features = list(train_transaction.columns[16:30])\nuniques = [len(train_transaction[col].unique()) for col in c_features]\nsns.set(font_scale=1.2)\nax = sns.barplot(c_features, uniques, log=True)\nax.set(xlabel='Feature', ylabel='log(unique count)', title='Number of unique values per feature TRAIN')\nfor p, uniq in zip(ax.patches, uniques):\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()/2.,\n            height + 10,\n            uniq,\n            ha=\"center\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del c_features, uniques\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**V features**"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(35, 8))\nv_features = list(train_transaction.columns[54:120])\nuniques = [len(train_transaction[col].unique()) for col in v_features]\nsns.set(font_scale=1.2)\nax = sns.barplot(v_features, uniques, log=True)\nax.set(xlabel='Feature', ylabel='log(unique count)', title='Number of unique values per feature')\nfor p, uniq in zip(ax.patches, uniques):\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()/2.,\n            height + 10,\n            uniq,\n            ha=\"center\") ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del v_features, uniques\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(35, 8))\nv_features = list(train_transaction.columns[120:170])\nuniques = [len(train_transaction[col].unique()) for col in v_features]\nsns.set(font_scale=1.2)\nax = sns.barplot(v_features, uniques, log=True)\nax.set(xlabel='Feature', ylabel='log(unique count)', title='Number of unique values per feature')\nfor p, uniq in zip(ax.patches, uniques):\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()/2.,\n            height + 10,\n            uniq,\n            ha=\"center\") ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del v_features, uniques\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(35, 8))\nv_features = list(train_transaction.columns[170:220])\nuniques = [len(train_transaction[col].unique()) for col in v_features]\nsns.set(font_scale=1.2)\nax = sns.barplot(v_features, uniques, log=True)\nax.set(xlabel='Feature', ylabel='log(unique count)', title='Number of unique values per feature')\nfor p, uniq in zip(ax.patches, uniques):\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()/2.,\n            height + 10,\n            uniq,\n            ha=\"center\") ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del v_features, uniques\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(35, 8))\nv_features = list(train_transaction.columns[220:270])\nuniques = [len(train_transaction[col].unique()) for col in v_features]\nsns.set(font_scale=1.2)\nax = sns.barplot(v_features, uniques, log=True)\nax.set(xlabel='Feature', ylabel='log(unique count)', title='Number of unique values per feature')\nfor p, uniq in zip(ax.patches, uniques):\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()/2.,\n            height + 10,\n            uniq,\n            ha=\"center\") ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del v_features, uniques\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(35, 8))\nv_features = list(train_transaction.columns[270:320])\nuniques = [len(train_transaction[col].unique()) for col in v_features]\nsns.set(font_scale=1.2)\nax = sns.barplot(v_features, uniques, log=True)\nax.set(xlabel='Feature', ylabel='log(unique count)', title='Number of unique values per feature')\nfor p, uniq in zip(ax.patches, uniques):\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()/2.,\n            height + 10,\n            uniq,\n            ha=\"center\") ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del v_features, uniques\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(38, 8))\nv_features = list(train_transaction.columns[320:390])\nuniques = [len(train_transaction[col].unique()) for col in v_features]\nsns.set(font_scale=1.2)\nax = sns.barplot(v_features, uniques, log=True)\nax.set(xlabel='Feature', ylabel='log(unique count)', title='Number of unique values per feature')\nfor p, uniq in zip(ax.patches, uniques):\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()/2.,\n            height + 10,\n            uniq,\n            ha=\"center\") ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del v_features, uniques\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**id_code**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_identity.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(35, 8))\nfeatures = list(train_identity.columns[0:38])\nuniques = [len(train_identity[col].unique()) for col in features]\nsns.set(font_scale=1.2)\nax = sns.barplot(features, uniques, log=True)\nax.set(xlabel='Feature', ylabel='log(unique count)', title='Number of unique values per feature TRAIN')\nfor p, uniq in zip(ax.patches, uniques):\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()/2.,\n            height + 10,\n            uniq,\n            ha=\"center\") ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del features, uniques\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Categorical Features**\n\n- ProductCD\n- emaildomain\n- card1 - card6\n- addr1, addr2\n- P_emaildomain\n- R_emaildomain\n- M1 - M9\n- DeviceType\n- DeviceInfo\n- id_12 - id_38"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_transaction.head(6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_identity.head(6)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**ProductCD**"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1, 2, figsize=(20,5))\n\nsns.countplot(x=\"ProductCD\", ax=ax[0], hue = \"isFraud\", data=train_transaction)\nax[0].set_title('ProductCD train', fontsize=14)\nsns.countplot(x=\"ProductCD\", ax=ax[1], data=test_transaction)\nax[1].set_title('ProductCD test', fontsize=14)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Device Type & Device Info**"},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.countplot(x=\"DeviceType\", data=train_identity)\nax.set_title('DeviceType', fontsize=14)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Device information**"},{"metadata":{"trusted":true},"cell_type":"code","source":"print (\"Unique Devices = \",train_identity['DeviceInfo'].nunique())\ntrain_identity['DeviceInfo'].value_counts().head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Card**"},{"metadata":{"trusted":true},"cell_type":"code","source":"cards = ['card1', 'card2', 'card3', 'card4', 'card5', 'card6']\nfor i in cards:\n    print (\"Unique \",i, \" = \",train_transaction[i].nunique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1, 4, figsize=(25,5))\n\nsns.countplot(x=\"card4\", ax=ax[0], data=train_transaction.loc[train_transaction['isFraud'] == 0])\nax[0].set_title('card4 isFraud=0', fontsize=14)\nsns.countplot(x=\"card4\", ax=ax[1], data=train_transaction.loc[train_transaction['isFraud'] == 1])\nax[1].set_title('card4 isFraud=1', fontsize=14)\nsns.countplot(x=\"card6\", ax=ax[2], data=train_transaction.loc[train_transaction['isFraud'] == 0])\nax[2].set_title('card6 isFraud=0', fontsize=14)\nsns.countplot(x=\"card6\", ax=ax[3], data=train_transaction.loc[train_transaction['isFraud'] == 1])\nax[3].set_title('card6 isFraud=1', fontsize=14)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Email Domain**"},{"metadata":{"trusted":true},"cell_type":"code","source":"\"emaildomain\" in train_transaction.columns, \"emaildomain\" in train_identity.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1, 3, figsize=(32,10))\n\nsns.countplot(y=\"P_emaildomain\", ax=ax[0], data=train_transaction)\nax[0].set_title('P_emaildomain', fontsize=14)\nsns.countplot(y=\"P_emaildomain\", ax=ax[1], data=train_transaction.loc[train_transaction['isFraud'] == 1])\nax[1].set_title('P_emaildomain isFraud = 1', fontsize=14)\nsns.countplot(y=\"P_emaildomain\", ax=ax[2], data=train_transaction.loc[train_transaction['isFraud'] == 0])\nax[2].set_title('P_emaildomain isFraud = 0', fontsize=14)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1, 3, figsize=(32,10))\n\nsns.countplot(y=\"R_emaildomain\", ax=ax[0], data=train_transaction)\nax[0].set_title('R_emaildomain', fontsize=14)\nsns.countplot(y=\"R_emaildomain\", ax=ax[1], data=train_transaction.loc[train_transaction['isFraud'] == 1])\nax[1].set_title('R_emaildomain isFraud = 1', fontsize=14)\nsns.countplot(y=\"R_emaildomain\", ax=ax[2], data=train_transaction.loc[train_transaction['isFraud'] == 0])\nax[2].set_title('R_emaildomain isFraud = 0', fontsize=14)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Il semble que les criminels préfèrent le gmail.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"del fig, ax\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <a style=\"color:#6699ff\"> III. Preprocessing</a>"},{"metadata":{},"cell_type":"markdown","source":"## Merge transaction & identity "},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntrain_df = pd.merge(train_transaction, train_identity, on = \"TransactionID\", how = \"left\")\nprint(\"Tain: \",train_df.shape)\ndel train_transaction, train_identity\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntest_df = pd.merge(test_transaction, test_identity, on = \"TransactionID\", how = \"left\")\nprint(\"Test: \",test_df.shape)\ntest_df[\"isFraud\"] = 0\ndel test_transaction, test_identity\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Pipeline of preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"emails = {\n'gmail': 'google', \n'att.net': 'att', \n'twc.com': 'spectrum', \n'scranton.edu': 'other', \n'optonline.net': 'other', \n'hotmail.co.uk': 'microsoft',\n'comcast.net': 'other', \n'yahoo.com.mx': 'yahoo', \n'yahoo.fr': 'yahoo',\n'yahoo.es': 'yahoo', \n'charter.net': 'spectrum', \n'live.com': 'microsoft', \n'aim.com': 'aol', \n'hotmail.de': 'microsoft', \n'centurylink.net': 'centurylink',\n'gmail.com': 'google', \n'me.com': 'apple', \n'earthlink.net': 'other', \n'gmx.de': 'other',\n'web.de': 'other', \n'cfl.rr.com': 'other', \n'hotmail.com': 'microsoft', \n'protonmail.com': 'other', \n'hotmail.fr': 'microsoft', \n'windstream.net': 'other', \n'outlook.es': 'microsoft', \n'yahoo.co.jp': 'yahoo', \n'yahoo.de': 'yahoo',\n'servicios-ta.com': 'other', \n'netzero.net': 'other', \n'suddenlink.net': 'other',\n'roadrunner.com': 'other', \n'sc.rr.com': 'other', \n'live.fr': 'microsoft',\n'verizon.net': 'yahoo', \n'msn.com': 'microsoft', \n'q.com': 'centurylink', \n'prodigy.net.mx': 'att', \n'frontier.com': 'yahoo', \n'anonymous.com': 'other', \n'rocketmail.com': 'yahoo',\n'sbcglobal.net': 'att',\n'frontiernet.net': 'yahoo', \n'ymail.com': 'yahoo',\n'outlook.com': 'microsoft',\n'mail.com': 'other', \n'bellsouth.net': 'other',\n'embarqmail.com': 'centurylink',\n'cableone.net': 'other', \n'hotmail.es': 'microsoft', \n'mac.com': 'apple',\n'yahoo.co.uk': 'yahoo',\n'netzero.com': 'other', \n'yahoo.com': 'yahoo', \n'live.com.mx': 'microsoft',\n'ptd.net': 'other',\n'cox.net': 'other',\n'aol.com': 'aol',\n'juno.com': 'other',\n'icloud.com': 'apple'\n}\n\n# number types for filtering the columns\nint_types = [\"int8\", \"int16\", \"int32\", \"int64\", \"float\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's check how many missing values has each column.\n\ndef check_nan(df, limit):\n    '''\n    Check how many values are missing in each column.\n    If the number of missing values are higher than limit, we drop the column.\n    '''\n    \n    total_rows = df.shape[0]\n    total_cols = df.shape[1]\n    \n    total_dropped = 0\n    col_to_drop = []\n    \n    for col in df.columns:\n\n        null_sum = df[col].isnull().sum()\n        perc_over_total = round((null_sum/total_rows), 2)\n        \n        if perc_over_total > limit:\n            \n            print(\"The col {} contains {} null values.\\nThis represents {} of total rows.\"\\\n                  .format(col, null_sum, perc_over_total))\n            \n            print(\"Dropping column {} from the df.\\n\".format(col))\n            \n            col_to_drop.append(col)\n            total_dropped += 1            \n    \n    df.drop(col_to_drop, axis = 1, inplace = True)\n    print(\"We have dropped a total of {} columns.\\nIt's {} of the total\"\\\n          .format(total_dropped, round((total_dropped/total_cols), 2)))\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def binarizer(df_train, df_test):\n    '''\n    Work with cat features and binarize the values.\n    Works with 2 dataframes at a time and returns a tupple of both.\n    '''\n    cat_cols = df_train.select_dtypes(exclude=int_types).columns\n\n    for col in cat_cols:\n        \n        # creating a list of unique features to binarize so we dont get and value error\n        unique_train = list(df_train[col].unique())\n        unique_test = list(df_test[col].unique())\n        unique_values = list(set(unique_train + unique_test))\n        \n        enc = LabelEncoder()\n        enc.fit(unique_values)\n        \n        df_train[col] = enc.transform((df_train[col].values).reshape(-1 ,1))\n        df_test[col] = enc.transform((df_test[col].values).reshape(-1 ,1))\n    \n    return (df_train, df_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def cathegorical_imputer(df_train, df_test, strategy, fill_value):\n    '''\n    Replace all cathegorical features with a constant or the most frequent strategy.\n    '''\n    cat_cols = df_train.select_dtypes(exclude=int_types).columns\n    \n    for col in cat_cols:\n        print(\"Working with column {}\".format(col))\n        \n        # select the correct inputer\n        if strategy == \"constant\":\n            # input a fill_value of -999 to all nulls\n            inputer = SimpleImputer(strategy=strategy, fill_value=fill_value)\n        elif strategy == \"most_frequent\":\n            inputer = SimpleImputer(strategy=strategy)\n        \n        # replace the nulls in train and test\n        df_train[col] = inputer.fit_transform(X = (df_train[col].values).reshape(-1, 1))\n        df_test[col] = inputer.transform(X = (df_test[col].values).reshape(-1, 1))\n        \n    return (df_train, df_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def numerical_inputer(df_train, df_test, strategy, fill_value):\n    '''\n    Replace NaN in the numerical features.\n    Works with 2 dataframes at a time (train & test).\n    Return a tupple of both.\n    '''\n    \n    # assert valid strategy\n    message = \"Please select a valid strategy (mean, median, constant (and give a fill_value) or most_frequent)\"\n    assert strategy in [\"constant\", \"most_frequent\", \"mean\", \"median\"], message\n    \n    # int_types defined earlier in the kernel\n    num_cols = df_train.select_dtypes(include = int_types).columns\n    \n    for col in num_cols:\n\n        print(\"Working with column {}\".format(col))\n\n        # select the correct inputer\n        if strategy == \"constant\":\n            inputer = SimpleImputer(strategy=strategy, fill_value=fill_value)\n        elif strategy == \"most_frequent\":\n            inputer = SimpleImputer(strategy=strategy)\n        elif strategy == \"mean\":\n            inputer = SimpleImputer(strategy=strategy)\n        elif strategy == \"median\":\n            inputer = SimpleImputer(strategy=strategy)\n\n        # replace the nulls in train and test\n        try:\n            df_train[col] = inputer.fit_transform(X = (df_train[col].values).reshape(-1, 1))\n            df_test[col] = inputer.transform(X = (df_test[col].values).reshape(-1, 1))\n        except:\n            print(\"Col {} gave and error.\".format(col))\n            \n    return (df_train, df_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def pipeline(df_train, df_test):\n    '''\n    We define a personal pipeline to process the data and fill with processing functions.\n    NOTE: modifies the df in place.\n    '''\n    print(\"Shape of train is {}\".format(df_train.shape))\n    print(\"Shape of test is {}\".format(df_test.shape))\n    # We have set the limit of 70%. If a column contains more that 70% of it's values as NaN/Missing values we will drop the column\n    # Since it's very unlikely that it will help our future model.\n    print(\"Checking for nan values\\n\")\n    df_train = check_nan(df_train, limit=0.7)\n    \n    # Select the columns from df_train with less nulls and asign to test.\n    df_test = df_test[list(df_train.columns)]\n          \n    print(\"Shape of train is {}\".format(df_train.shape))\n    print(\"Shape of test is {}\".format(df_test.shape))\n          \n    # mapping emails\n    print(\"Mapping emails \\n\")\n    df_train[\"EMAILP\"] = df_train[\"P_emaildomain\"].map(emails)\n    df_test[\"EMAILP\"] = df_test[\"P_emaildomain\"].map(emails)\n\n    print(\"Shape of train is {}\".format(df_train.shape))\n    print(\"Shape of test is {}\".format(df_test.shape))\n          \n    # replace nulls from the train and test df with a value of \"Other\"\n    print(\"Working with cathegorical values\\n\")\n    df_train, df_test = cathegorical_imputer(df_train, df_test, strategy = \"constant\", fill_value = \"Other\")\n    \n    print(\"Shape of train is {}\".format(df_train.shape))\n    print(\"Shape of test is {}\".format(df_test.shape))\n          \n    # now we will make a one hot encoder of these colums\n    print(\"Binarazing values\\n\")\n    df_train, df_test = binarizer(df_train, df_test)\n    \n    print(\"Shape of train is {}\".format(df_train.shape))\n    print(\"Shape of test is {}\".format(df_test.shape))\n          \n    # working with null values in numeric columns\n    print(\"Working with numerical columns. NAN values\\n\")\n    df_train, df_test = numerical_inputer(df_train, df_test, strategy = \"constant\", fill_value=-999)\n        \n    print(\"Shape of train is {}\".format(df_train.shape))\n    print(\"Shape of test is {}\".format(df_test.shape))\n          \n    return (df_train, df_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# before preprocesing\nprint(\"Train before preprocesing: \",train_df.shape)\nprint(\"Test before preprocesing: \",test_df.shape)\n\ntrain_df, test_df = pipeline(train_df, test_df)\n\n# after preprocesing\nprint(\"Train after preprocesing: \",train_df.shape)\nprint(\"Test after preprocesing: \",test_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check for null values\ncolumns = train_df.columns\nfor col in  columns:\n    total_nulls = train_df[col].isnull().sum()\n    if total_nulls > 0:\n        print(col, total_nulls)\n        \ncolumns = test_df.select_dtypes(exclude=int_types).columns\ntrain_df[columns]\n\ncolumns = test_df.select_dtypes(include=int_types).columns\ntrain_df[columns]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Reduce Memory Usage**"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# From kernel https://www.kaggle.com/gemartin/load-data-reduce-memory-usage\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntrain_df = reduce_mem_usage(train_df)\ntest_df = reduce_mem_usage(test_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <a style=\"color:#6699ff\"> IV. Machine Learning Models</a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"cols_to_drop = [\"TransactionID\", \"isFraud\", \"TransactionDT\"]\nuseful_cols = list(train_df.columns)\n\nfor col in cols_to_drop:\n    while True:\n        try:\n            useful_cols.remove(col)\n        except:\n            break\n            \nY = train_df[\"isFraud\"]\nX = train_df[useful_cols]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Reduce Memory Usage**"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nX_train = reduce_mem_usage(X_train)\nX_test = reduce_mem_usage(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## XGBoost classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"eval_set = [(X_train, y_train), (X_test, y_test)]\nxgboost_classifier = XGBClassifier()\nxgboost_classifier.fit(X_train, y_train, eval_metric=[\"error\", \"logloss\"], eval_set=eval_set, verbose=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = xgboost_classifier.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(confusion_matrix(predictions, y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(predictions, y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Learning curve for XGBoost**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# retrieve performance metrics\nresults = xgboost_classifier.evals_result()\nepochs = len(results['validation_0']['error'])\nx_axis = range(0, epochs)\n# plot log loss\nfig, ax = plt.subplots()\nax.plot(x_axis, results['validation_0']['logloss'], label='Train')\nax.plot(x_axis, results['validation_1']['logloss'], label='Test')\nax.legend()\nplt.ylabel('Log Loss')\nplt.title('XGBoost Log Loss')\nplt.show()\n# plot classification error\nfig, ax = plt.subplots()\nax.plot(x_axis, results['validation_0']['error'], label='Train')\nax.plot(x_axis, results['validation_1']['error'], label='Test')\nax.legend()\nplt.ylabel('Classification Error')\nplt.title('XGBoost Classification Error')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Feature importance for XGBoost**"},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = list(train_df.columns)\nfeature_imp = pd.DataFrame(sorted(zip(xgboost_classifier.feature_importances_,cols)), columns=['Value','Feature'])\nplt.figure(figsize=(20, 10))\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False).iloc[:50])\nplt.title('XGBoost hyperparameter tunning Most Important Features')\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**First submission for XGBOOST**"},{"metadata":{"trusted":true},"cell_type":"code","source":"proba = xgboost_classifier.predict_proba(test_df[useful_cols])\nproba[:,1]\n\ny_preds = proba[:,1]\n\nsubmission = pd.read_csv('sample_submission.csv', index_col='TransactionID')\nsubmission['isFraud'] = y_preds\nsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(submission.isFraud,bins=100)\nplt.ylim((0,5000))\nplt.title('XGBoost submission')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('/kaggle/working/xgboost_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del eval_set, xgboost_classifier, predictions, results, epochs, x_axis, fig, ax, cols, feature_imp, proba, y_preds, submission\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## XGBoost classifier with hyperparameter tuning "},{"metadata":{"trusted":true},"cell_type":"code","source":"xgboost_classifier = XGBClassifier(n_estimators=5000,\n        max_depth=12, \n        learning_rate=0.02, \n        subsample=0.8,\n        colsample_bytree=0.4, \n        missing=-1,\n        tree_method='gpu_hist')\n\neval_set = [(X_train, y_train), (X_test, y_test)]\nxgboost_classifier.fit(X_train, y_train, eval_metric=[\"error\", \"logloss\"], eval_set=eval_set, verbose=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del eval_set\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = xgboost_classifier.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(confusion_matrix(predictions, y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(predictions, y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del predictions\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Learning curve for XGBoost hyperparameter tunning**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# retrieve performance metrics\nresults = xgboost_classifier.evals_result()\nepochs = len(results['validation_0']['error'])\nx_axis = range(0, epochs)\n# plot log loss\nfig, ax = plt.subplots()\nax.plot(x_axis, results['validation_0']['logloss'], label='Train')\nax.plot(x_axis, results['validation_1']['logloss'], label='Test')\nax.legend()\nplt.ylabel('Log Loss')\nplt.title('XGBoost tunning Log Loss')\nplt.show()\n# plot classification error\nfig, ax = plt.subplots()\nax.plot(x_axis, results['validation_0']['error'], label='Train')\nax.plot(x_axis, results['validation_1']['error'], label='Test')\nax.legend()\nplt.ylabel('Classification Error')\nplt.title('XGBoost tunning Classification Error')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del results, epochs, x_axis, fig, ax","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Feature importance for XGBoost hyperparameter tunning**"},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = list(train_df.columns)\nfeature_imp = pd.DataFrame(sorted(zip(xgboost_classifier.feature_importances_,cols)), columns=['Value','Feature'])\nplt.figure(figsize=(20, 10))\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False).iloc[:50])\nplt.title('XGBoost hyperparameter tunning Most Important Features')\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del cols, feature_imp\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Submission for XGBOOST with hyperparameter tunning**"},{"metadata":{"trusted":true},"cell_type":"code","source":"proba = xgboost_classifier.predict_proba(test_df[useful_cols])\nproba[:,1]\n\ny_preds = proba[:,1]\n\nsubmission = pd.read_csv('sample_submission.csv', index_col='TransactionID')\nsubmission['isFraud'] = y_preds\nsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(submission.isFraud,bins=100)\nplt.ylim((0,5000))\nplt.title('XGBoost hyperparameter tunning submission')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('/kaggle/working/xgboost_tunning_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del xgboost_classifier,proba, y_preds, submission\ngc.collect()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}